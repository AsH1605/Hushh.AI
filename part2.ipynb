{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests pydantic transformers sentence-transformers scikit-learn google-generativeai==0.5.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json, re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_gemini(text: str):\n",
    "    \"\"\"\n",
    "    Process input text using the Gemini API and return a structured JSON output.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Process the following text and return the output in the following JSON structure:\n",
    "    {{\n",
    "      \"title\": \"Concise title summarizing the content\",\n",
    "      \"key_points\": [\"Point 1\", \"Point 2\", \"...\"],\n",
    "      \"sentiment\": \"Positive/Neutral/Negative\",\n",
    "      \"entities\": [\"Entity1\", \"Entity2\", \"...\"]\n",
    "    }}\n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt\n",
    "        )\n",
    "        result = response.text\n",
    "\n",
    "        title_match = re.search(r'\"title\":\\s*\"([^\"]+)\"', result)\n",
    "        title = title_match.group(1) if title_match else \"N/A\"\n",
    "\n",
    "        key_points_match = re.search(r'\"key_points\":\\s*\\[(.*?)\\]', result)\n",
    "        key_points = [point.strip().strip('\"') for point in key_points_match.group(1).split(\",\")] if key_points_match else []\n",
    "\n",
    "        sentiment_match = re.search(r'\"sentiment\":\\s*\"([^\"]+)\"', result)\n",
    "        sentiment = sentiment_match.group(1) if sentiment_match else \"N/A\"\n",
    "\n",
    "        entities_match = re.search(r'\"entities\":\\s*\\[(.*?)\\]', result)\n",
    "        entities = [entity.strip().strip('\"') for entity in entities_match.group(1).split(\",\")] if entities_match else []\n",
    "\n",
    "        json_output = {\n",
    "            \"title\": title,\n",
    "            \"key_points\": key_points,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "        return json.dumps(json_output, indent=4)  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing with Gemini API: {e}\")\n",
    "        return json.dumps({\"error\": \"Failed to process text\", \"details\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "\n",
    "class ProcessedData(BaseModel):\n",
    "    title: str\n",
    "    key_points: List[str]\n",
    "    sentiment: str\n",
    "    entities: List[str]\n",
    "\n",
    "def validate_response(data: dict):\n",
    "    try:\n",
    "        processed = ProcessedData(**data)\n",
    "        return processed\n",
    "    except ValidationError as e:\n",
    "        print(\"Validation Error:\", e.json())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def process_with_local_model(text: str):\n",
    "    model_pipeline = pipeline(\"text2text-generation\", model=\"your-local-model-path\")\n",
    "    response = model_pipeline(\n",
    "        f\"Process the following text: {text}\",\n",
    "        max_length=1024,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def api_request_with_retry(request_func, *args, retries=3, backoff_factor=0.3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return request_func(*args)\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(backoff_factor * (2 ** attempt) + random.uniform(0, 0.1))\n",
    "            else:\n",
    "                print(\"API request failed:\", e)\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def compare_outputs(api_output, local_output):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    api_embedding = model.encode(\" \".join(api_output.get(\"key_points\", [])))\n",
    "    local_embedding = model.encode(\" \".join(local_output.get(\"key_points\", [])))\n",
    "\n",
    "    similarity = cosine_similarity([api_embedding], [local_embedding])[0][0]\n",
    "    print(\"Semantic Similarity:\", similarity)\n",
    "\n",
    "    print(\"API Sentiment:\", api_output.get(\"sentiment\"))\n",
    "    print(\"Local Sentiment:\", local_output.get(\"sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing with Local Model\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "model_name = \"t5-large\"\n",
    "model_path=\"./local_model_large\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    # model_pipeline = pipeline(\"text2text-generation\", model=\"tuner007/pegasus_paraphrase_large\")\n",
    "    # model_pipeline.save_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "\n",
    "def process_with_local_model(text: str):\n",
    "    model_pipeline = pipeline(\"text2text-generation\", model=\"./local_model_large\")\n",
    "    prompt = get_prompt(text)\n",
    "    # print(prompt)\n",
    "    response = model_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        # max_length=1024,\n",
    "        # temperature=0.7\n",
    "    )\n",
    "    # print(response)\n",
    "    return response[0]['generated_text']\n",
    "# Validating the generated output\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "\n",
    "class ProcessedData(BaseModel):\n",
    "    title: str\n",
    "    key_points: List[str]\n",
    "    sentiment: str\n",
    "    entities: List[str]\n",
    "\n",
    "def validate_response(data: dict):\n",
    "    try:\n",
    "        processed = ProcessedData(**data)\n",
    "        return processed\n",
    "    except ValidationError as e:\n",
    "        print(\"Validation Error:\", e.json())\n",
    "        return None\n",
    "# Comparing outputs\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def api_request_with_retry(request_func, *args, retries=3, backoff_factor=0.3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return request_func(*args)\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(backoff_factor * (2 ** attempt) + random.uniform(0, 0.1))\n",
    "            else:\n",
    "                print(\"API request failed:\", e)\n",
    "                return None\n",
    "            \n",
    "\n",
    "def compare_outputs(api_output, local_output):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    api_embedding = model.encode(\" \".join(api_output.get(\"key_points\", [])))\n",
    "    local_embedding = model.encode(\" \".join(local_output.get(\"key_points\", [])))\n",
    "\n",
    "    similarity = cosine_similarity([api_embedding], [local_embedding])[0][0]\n",
    "    print(\"Semantic Similarity:\", similarity)\n",
    "\n",
    "    print(\"API Sentiment:\", api_output.get(\"sentiment\"))\n",
    "    print(\"Local Sentiment:\", local_output.get(\"sentiment\"))\n",
    "# Runner code\n",
    "if __name__ == \"__main__\":\n",
    "    raw_text = \"Apple announced the launch of its new iPhone 15 today. Fans are excited about its enhanced camera and longer battery life. Critics, however, feel that the price point is too high.\"\n",
    "\n",
    "    gemini_output = process_with_gemini(raw_text)\n",
    "    print(\"Gemini Output:\")\n",
    "    print(gemini_output)\n",
    "    if isinstance(gemini_output, str):\n",
    "        gemini_output = json.loads(gemini_output) \n",
    "\n",
    "    validated_gemini_output = validate_response(gemini_output)\n",
    "\n",
    "    # local_output = gemini_output\n",
    "    local_output = process_with_local_model(raw_text)\n",
    "    print(\"Local Output:\")\n",
    "    print(local_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
